{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 4 - Questions\n",
    "## Q1\n",
    "### a)\n",
    "The main difference between the 'convolutions' in CNNs and the convolutions in other computer vision algorithms is through the construction of the kernel filters. In most computer vision algorithms, the kernel is constructed by the user in order to perform a specific task (e.g. Harris Corner Detector to find corners in the image). On the other hand, CNNs aim to learn the weights of the kernel filter, where the functionality of the filter is not necessarily known to an outside observer.\n",
    "### b)\n",
    "The main advantage of user-constructed filters is its algorithmic applicability. That is to say, because the filter and its functionality is known, different kernels can be selectively chosen to perform specific tasks depending on the algorithm. In the SIFT algorithm, kernel filters are used to extract corners in an image, which are used to represent fundamental features of the image. However, in many cases, human-defined characterizations of what features are fundamental to an image tend to not be effective. This is the main advantage of CNNs. By simply defining a loss and an architecture, we allow a network to self-learn the fundamental features of an image, leading to locally optimal results for the given architecture. Of course, the main disadvantage of this approach is the lack of understandability for a human interpreter, thus making neural network approaches 'black box' approaches.\n",
    "## Q2\n",
    "A locally-connected MLP likely will have worse results compared to one that's densely connected. Each perceptron in a locally connected MLP only takes in as input the outputs of the perceptrons closest to it in the previous layer, in contrast to a densely connected layer which takes all previous outputs. Although a locally connected MLP may require fewer operations, resulting in a faster network, it is likely that just the locally connected features are insufficient for making a classification. In a famous example, a group of blind men who have never come across an elephant try to imagine what an elephant is like by touching it. However, they can only touch one body part. Unsurprisingly, each man came to a different conclusion about the elephant depending on the body part: the man who touched the trunk compared it to a snake, the man who touched the leg compared it to a tree, etc. We can think of each of these blind men as the learned convolutional feature maps in a CNN, where each convolutional filter learns a different feature of the image. A densely connected layer is important for taking the global context from every convolutional filter in order to make an observation.\n",
    "## Q3\n",
    "Learning rate, batch size, and training time are all important hyperparameters to select when training a neural network. Learning rate denotes the step size for the algorithm to use when moving along the gradient towards to global minimum. A large step size may result in the network overstepping the global minimum and never being able to find the true optimum. On the other hand, a small step size is much less likely to overshoot the global/local minimum but will result in a much slower training speed, since more steps will be required to converge. Batch size denotes the subset of the training data used to train the model during each epoch of training. Generally large batch sizes are preferred since they allow faster computations through GPU parallelization, although this takes a lot of memory. Using a larger batch size also guarantees convergence to the global optima of the objective function. However, it is found empirically that too large of a batch size leads to slower convergence to the optima as well as poorer generalization of the network. On the other hand, smaller batch sizes allow for faster convergence to a local optima and good generalization, at the cost of likely not converging to the global maxima of the objective function. Lastly, training time is used to denote the number of epochs that the network trains on the dataset. It is necessary to have training time long enough such that the network may learn the dataset, however too long training times will result in overfitting, or memorization of the data, and thus poor generalization.\n",
    "## Q4\n",
    "Effect of $1\\times 1\\times d$ max pooling layer\n",
    "- Increases computational cost of training                  F\n",
    "- Decreases computational cost of training                  T\n",
    "- Increases computational cost of testing                   F\n",
    "- Decreases computational cost of testing                   T\n",
    "- Increases overfitting                                     F\n",
    "- Decreases overfitting                                     T\n",
    "- Increases underfitting                                    T\n",
    "- Decreases underfitting                                    F\n",
    "- Increases the nonlinearity of the decision function       T\n",
    "- Decreases the nonlinearity of the decision function       F\n",
    "- Provides local rotational invariance                      F\n",
    "- Provides global rotational invariance                     T\n",
    "- Provides local scale invariance                           F\n",
    "- Provides global scale invariance                          T\n",
    "- Provides local translational invariance                   F\n",
    "- Provides global translational invariance                  T\n",
    "\n",
    "## Q5\n",
    "The single layer perceptron with 10 neurons is implemented as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from code import hyperparameters as hp\n",
    "\n",
    "def train_nn(self):\n",
    "     # This is our training data as indices into an image storage array\n",
    "    indices = list(range(self.train_images.shape[0]))\n",
    "\n",
    "    # These are our storage variables for our update gradients.\n",
    "    # delta_W is the matrix of gradients on the weights of our neural network\n",
    "    #     Each row is a different neuron (with its own weights)\n",
    "    # delta_b is the vector of gradients on the biases (one per neuron)\n",
    "    delta_W = np.zeros((self.input_size, self.num_classes))\n",
    "    delta_b = np.zeros((1, self.num_classes))\n",
    "\n",
    "    # Iterate over the number of epochs declared in the hyperparameters.py file\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        # Overall per-epoch sum of the loss\n",
    "        loss_sum = 0\n",
    "        # Shuffle the data before training each epoch to remove ordering bias\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # For each image in the datset:\n",
    "        for index in range(len(indices)):\n",
    "            # Get input training image and ground truth label\n",
    "            i = indices[index]\n",
    "            img = self.train_images[i]\n",
    "            gt_label = self.train_labels[i]\n",
    "\n",
    "            ################\n",
    "            # GENERAL ADVICE:\n",
    "            # This is _precise work_ - we need very few lines of code.\n",
    "            # At this point, we need not write any for loops.\n",
    "            # As a guide, the solution for this can be writtein in 14 lines of code.\n",
    "            #\n",
    "            # Further, please do not use any library functions.\n",
    "\n",
    "            ################\n",
    "            # FORWARD PASS:\n",
    "            # This is where we take our current estimate of the weights and biases\n",
    "            # and compute the current error against the training data.\n",
    "\n",
    "            # Step 1:\n",
    "            # Compute the output response to this 'img' input for each neuron (linear unit).\n",
    "            # Our current estimate for the weights and biases are stored in:\n",
    "            #    self.W\n",
    "            #    self.b\n",
    "            # Remember: use matrix operations.\n",
    "            # Our output will be a number for each neuron stored in a vector.\n",
    "            out = np.matmul(img, self.W) + self.b\n",
    "\n",
    "            # Step 2:\n",
    "            # Convert these to probabilities by implementing the softmax function.\n",
    "            prob = np.exp(out) / (np.sum(np.exp(out)) + 1E-10)\n",
    "\n",
    "            # Step 3:\n",
    "            # Compute the error against the training label 'gt_label' using the cross-entropy loss\n",
    "            # Remember:\n",
    "            #     log has a potential divide by zero error\n",
    "            y = np.zeros(self.num_classes)\n",
    "            y[gt_label] = 1\n",
    "            loss_over_all_classes = - np.log(prob + 1E-10)\n",
    "            loss_sum = loss_sum + loss_over_all_classes[:, gt_label]\n",
    "\n",
    "\n",
    "            ################\n",
    "            # BACKWARD PASS (BACK PROPAGATION):\n",
    "            # This is where we find which direction to move in for gradient descent to\n",
    "            # optimize our weights and biases.\n",
    "            # Use the derivations from the questions handout.\n",
    "\n",
    "            # Step 4:\n",
    "            # Compute the delta_W and delta_b gradient terms for the weights and biases\n",
    "            # using the provided derivations in Eqs. 6 and 7 of the handout.\n",
    "            # Remember:\n",
    "            #    delta_W is a matrix the size of the input by the size of the classes\n",
    "            #    delta_b is a vector\n",
    "            # Note:\n",
    "            #    By equation 6 and 7, we need to subtract 1 from p_j only if it is\n",
    "            #    the true class probability.\n",
    "            delta_b = prob - y\n",
    "            delta_W = np.outer(img, delta_b)\n",
    "\n",
    "\n",
    "            # Step 5:\n",
    "            # Update self.W and self.b using the gradient terms\n",
    "            # and the self.learning_rate hyperparameter.\n",
    "            # Eqs. 4 and 5 in the handout.\n",
    "            self.b = self.b - self.learning_rate * delta_b\n",
    "            self.W = self.W - self.learning_rate * delta_W"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
