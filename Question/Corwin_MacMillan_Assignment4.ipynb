{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 4 - Questions\n",
    "## Q1\n",
    "### a)\n",
    "The main difference between the 'convolutions' in CNNs and the convolutions in other computer vision algorithms is through the construction of the kernel filters. In most computer vision algorithms, the kernel is constructed by the user in order to perform a specific task (e.g. Harris Corner Detector to find corners in the image). On the other hand, CNNs aim to learn the weights of the kernel filter, where the functionality of the filter is not necessarily known to an outside observer.\n",
    "### b)\n",
    "The main advantage of user-constructed filters is its algorithmic applicability. That is to say, because the filter and its functionality is known, different kernels can be selectively chosen to perform specific tasks depending on the algorithm. In the SIFT algorithm, kernel filters are used to extract corners in an image, which are used to represent fundamental features of the image. However, in many cases, human-defined characterizations of what features are fundamental to an image tend to not be effective. This is the main advantage of CNNs. By simply defining a loss and an architecture, we allow a network to self-learn the fundamental features of an image, leading to locally optimal results for the given architecture. Of course, the main disadvantage of this approach is the lack of understandability for a human interpreter, thus making neural network approaches 'black box' approaches.\n",
    "## Q2\n",
    "A locally-connected MLP likely will have worse results compared to one that's densely connected. Each perceptron in a locally connected MLP only takes in as input the outputs of the perceptrons closest to it in the previous layer, in contrast to a densely connected layer which takes all previous outputs. Although a locally connected MLP may require fewer operations, resulting in a faster network, it is likely that just the locally connected features are insufficient for making a classification. In a famous example, a group of blind men who have never come across an elephant try to imagine what an elephant is like by touching it. However, they can only touch one body part. Unsurprisingly, each man came to a different conclusion about the elephant depending on the body part: the man who touched the trunk compared it to a snake, the man who touched the leg compared it to a tree, etc. We can think of each of these blind men as the learned convolutional feature maps in a CNN, where each convolutional filter learns a different feature of the image. A densely connected layer is important for taking the global context from every convolutional filter in order to make an observation.\n",
    "## Q3\n",
    "Learning rate, batch size, and training time are all important hyperparameters to select when training a neural network. Learning rate denotes the step size for the algorithm to use when moving along the gradient towards to global minimum. A large step size may result in the network overstepping the global minimum and never being able to find the true optimum. On the other hand, a small step size is much less likely to overshoot the global/local minimum but will result in a much slower training speed, since more steps will be required to converge. Batch size denotes the subset of the training data used to train the model during each epoch of training. Generally large batch sizes are preferred since they allow faster computations through GPU parallelization, although this takes a lot of memory. Using a larger batch size also guarantees convergence to the global optima of the objective function. However, it is found empirically that too large of a batch size leads to slower convergence to the optima as well as poorer generalization of the network. On the other hand, smaller batch sizes allow for faster convergence to a local optima and good generalization, at the cost of likely not converging to the global maxima of the objective function. Lastly, training time is used to denote the number of epochs that the network trains on the dataset. It is necessary to have training time long enough such that the network may learn the dataset, however too long training times will result in overfitting, or memorization of the data, and thus poor generalization.\n",
    "## Q4\n",
    "Effect of $1\\times 1\\times d$ max pooling layer\n",
    "- Increases computational cost of training                  F\n",
    "- Decreases computational cost of training                  T\n",
    "- Increases computational cost of testing                   F\n",
    "- Decreases computational cost of testing                   T\n",
    "\n",
    "- Increases overfitting                                     F\n",
    "- Decreases overfitting                                     T\n",
    "- Increases underfitting                                    T\n",
    "- Decreases underfitting                                    F\n",
    "\n",
    "- Increases the nonlinearity of the decision function       T\n",
    "- Decreases the nonlinearity of the decision function       F\n",
    "\n",
    "- Provides local rotational invariance                      T\n",
    "- Provides global rotational invariance                     T\n",
    "- Provides local scale invariance                           F\n",
    "- Provides global scale invariance                          T\n",
    "- Provides local translational invariance                   T\n",
    "- Provides global translational invariance                  T\n",
    "\n",
    "## Q5\n",
    "The single layer perceptron with 10 neurons is implemented as follows:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from code import hyperparameters as hp\n",
    "\n",
    "def train_nn(self):\n",
    "     # This is our training data as indices into an image storage array\n",
    "    indices = list(range(self.train_images.shape[0]))\n",
    "\n",
    "    # These are our storage variables for our update gradients.\n",
    "    # delta_W is the matrix of gradients on the weights of our neural network\n",
    "    #     Each row is a different neuron (with its own weights)\n",
    "    # delta_b is the vector of gradients on the biases (one per neuron)\n",
    "    delta_W = np.zeros((self.input_size, self.num_classes))\n",
    "    delta_b = np.zeros((1, self.num_classes))\n",
    "\n",
    "    # Iterate over the number of epochs declared in the hyperparameters.py file\n",
    "    for epoch in range(hp.num_epochs):\n",
    "        # Overall per-epoch sum of the loss\n",
    "        loss_sum = 0\n",
    "        # Shuffle the data before training each epoch to remove ordering bias\n",
    "        random.shuffle(indices)\n",
    "\n",
    "        # For each image in the datset:\n",
    "        for index in range(len(indices)):\n",
    "            # Get input training image and ground truth label\n",
    "            i = indices[index]\n",
    "            img = self.train_images[i]\n",
    "            gt_label = self.train_labels[i]\n",
    "\n",
    "            out = np.matmul(img, self.W) + self.b\n",
    "\n",
    "            prob = np.exp(out) / (np.sum(np.exp(out)) + 1E-10)\n",
    "\n",
    "            y = np.zeros(self.num_classes)\n",
    "            y[gt_label] = 1\n",
    "            loss_over_all_classes = - np.log(prob + 1E-10)\n",
    "            loss_sum += loss_over_all_classes[:, gt_label]\n",
    "\n",
    "            delta_b = prob - y\n",
    "            delta_W = np.outer(img, delta_b)\n",
    "\n",
    "            self.b -= self.learning_rate * delta_b\n",
    "            self.W -= self.learning_rate * delta_W"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following are the performances on the different datasets with and without the SVM. The batch size is chosen to\n",
    "be 4 to decrease runtime, the random weights for initializing the neurons is\n",
    "divided by 10 to prevent large calculations in the cross-entropy, and the\n",
    "learning rate is set to 0.01 for the same reason.\n",
    "#### NN on MNIST\n",
    "0 epochs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pan Lab\\Documents\\SYDE671ASS4\\Question\\code\n"
     ]
    }
   ],
   "source": [
    "%cd code"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [22324.35980181]\n",
      "nn model training accuracy: 91%\n"
     ]
    }
   ],
   "source": [
    "#0 epochs\n",
    "!python main.py -data mnist -mode nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [22361.69047881]\n",
      "Epoch 1: Total loss: [18944.67126444]\n",
      "Epoch 2: Total loss: [18158.54682276]\n",
      "Epoch 3: Total loss: [17764.28092771]\n",
      "Epoch 4: Total loss: [17609.96183968]\n",
      "Epoch 5: Total loss: [17373.16453262]\n",
      "Epoch 6: Total loss: [17325.65309289]\n",
      "Epoch 7: Total loss: [17155.56097669]\n",
      "Epoch 8: Total loss: [16978.27277752]\n",
      "Epoch 9: Total loss: [16876.34491097]\n",
      "nn model training accuracy: 92%\n"
     ]
    }
   ],
   "source": [
    "#9 epochs\n",
    "!python main.py -data mnist -mode nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The accuracy stays largely the same between 1 epoch and 10 epochs, despite the loss decreasing.\n",
    "This is likely because the learning rate is small, as well as the initial accuracy being immediately\n",
    "quite accurate.\n",
    "#### NN+SVM on MNIST"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [22194.22858588]\n",
      "nn+svm model training accuracy: 91%\n"
     ]
    }
   ],
   "source": [
    "# 0 epochs\n",
    "!python main.py -data mnist -mode nn+svm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [22349.5672069]\n",
      "Epoch 1: Total loss: [18873.20031704]\n",
      "Epoch 2: Total loss: [18183.25423491]\n",
      "Epoch 3: Total loss: [17787.47851326]\n",
      "Epoch 4: Total loss: [17570.2050617]\n",
      "Epoch 5: Total loss: [17345.27009409]\n",
      "Epoch 6: Total loss: [17247.77519733]\n",
      "Epoch 7: Total loss: [17139.15738445]\n",
      "Epoch 8: Total loss: [16968.56722361]\n",
      "Epoch 9: Total loss: [16945.29344995]\n",
      "nn+svm model training accuracy: 91%\n"
     ]
    }
   ],
   "source": [
    "#9 epochs\n",
    "!python main.py -data mnist -mode nn+svm\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As can be seen, performance doesn't improve with the SVM.\n",
    "This is likely because the dataset is very simple\n",
    "and a neural network is sufficient for classification.\n",
    "\n",
    "#### NN on scenerec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [11703.76179277]\n",
      "nn model training accuracy: 12%\n"
     ]
    }
   ],
   "source": [
    "# 0 epochs\n",
    "!python main.py -data scenerec -mode nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [11637.92271927]\n",
      "Epoch 1: Total loss: [10614.0663993]\n",
      "Epoch 2: Total loss: [10041.60088724]\n",
      "Epoch 3: Total loss: [9327.49023939]\n",
      "Epoch 4: Total loss: [8931.12291809]\n",
      "Epoch 5: Total loss: [8331.23461914]\n",
      "Epoch 6: Total loss: [7726.60796492]\n",
      "Epoch 7: Total loss: [7259.54056056]\n",
      "Epoch 8: Total loss: [7178.83979744]\n",
      "Epoch 9: Total loss: [6916.56043638]\n",
      "nn model training accuracy: 15%\n"
     ]
    }
   ],
   "source": [
    "# 9 epochs\n",
    "!python main.py -data scenerec -mode nn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, there is a slight increase in performance between\n",
    "0 epochs and 9 epochs, although performance remains\n",
    "relatively poor.\n",
    "#### NN+SVM on scenerec"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [11774.73438813]\n",
      "nn+svm model training accuracy: 24%\n"
     ]
    }
   ],
   "source": [
    "# 0 epochs\n",
    "!python main.py -data scenerec -mode nn+svm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Total loss: [11974.44627687]\n",
      "Epoch 1: Total loss: [10898.54619586]\n",
      "Epoch 2: Total loss: [9944.78743449]\n",
      "Epoch 3: Total loss: [9118.73405276]\n",
      "Epoch 4: Total loss: [8649.65384342]\n",
      "Epoch 5: Total loss: [8248.76335881]\n",
      "Epoch 6: Total loss: [8164.13652592]\n",
      "Epoch 7: Total loss: [7338.03317666]\n",
      "Epoch 8: Total loss: [7028.96348365]\n",
      "Epoch 9: Total loss: [7015.46219634]\n",
      "nn+svm model training accuracy: 21%\n"
     ]
    }
   ],
   "source": [
    "# 9 epochs\n",
    "!python main.py -data scenerec -mode nn+svm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this case, SVM shows improvement from the NN only\n",
    "case. However, further training of the model results in\n",
    "poorer performance, even though the performance is quite poor already.\n",
    "We can attribute the overall poor performanc eof the model to its lack\n",
    "of complexity, where the model is not complex enough to represent the complext\n",
    "dataset. This is in contrast to the simple MNIST dataset where 91% accuracy\n",
    "was easily achieved. The decrease in performance  while training can perhaps\n",
    "be attributed to random movement, where the network\n",
    "cannot find the proper gradient in the feature space and thus moves essenially\n",
    "random direction.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}